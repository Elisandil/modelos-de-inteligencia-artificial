{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7de1a213",
   "metadata": {},
   "source": [
    "# Tarea 3.2.- Preprocesamiento de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc66c48d",
   "metadata": {},
   "source": [
    "### **Detallar los pasos necesarios para el preprocesamiento de texto: tokenización, eliminación de stop words, lematización, etc. Entre 300 y 400 palabras**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4dc67",
   "metadata": {},
   "source": [
    "Cuando trabajamos con Procesamiento del Lenguaje Natural (PLN), los datos rara vez llegan listos para usar. El texto \"crudo\" es sucio, inconsistente y está lleno de ruido. Por eso, el preprocesamiento no es un paso opcional, sino la base que define la calidad de cualquier análisis posterior. Si alimentamos un modelo con basura, obtendremos basura.\n",
    "\n",
    "El proceso comienza casi siempre con la tokenización, es decir, el acto de romper el flujo continuo de escritura en unidades mínimas con significado, llamadas tokens. Aunque solemos pensar en palabras, un token puede ser también un signo de puntuación o un número. Sin esta segmentación, el ordenador solo vería una cadena interminable de caracteres sin sentido.\n",
    "\n",
    "Una vez dividido el texto, pasamos a la limpieza. Aquí eliminamos todo lo que no aporta valor semántico al problema que queremos resolver. Esto incluye borrar caracteres especiales, signos de puntuación extraños, emojis o URLs. El objetivo es reducir el ruido visual para que el algoritmo se centre exclusivamente en el mensaje.\n",
    "\n",
    "El siguiente filtro es la eliminación de \"stop words\" o palabras vacías. Son términos omnipresentes en el idioma (como \"el\", \"de\", \"para\", \"un\") que sirven de pegamento gramatical pero que apenas cargan significado por sí mismos. Al quitarlas, reducimos drásticamente el tamaño de los datos sin perder la esencia del contenido, lo cual acelera el procesamiento.\n",
    "\n",
    "Para normalizar el vocabulario, utilizamos la lematización. A diferencia de técnicas más rudimentarias que solo cortan el final de las palabras, la lematización analiza morfológicamente el término para devolverlo a su forma base o de diccionario (el lema). Así, \"comiendo\", \"comió\" y \"comeremos\" se unifican bajo el concepto \"comer\". Esto es crucial para que la máquina entienda que diferentes variantes representan la misma idea.\n",
    "\n",
    "Finalmente, dependiendo del objetivo, podemos realizar una filtrado por categoría gramatical. Si buscamos analizar temas, quizá solo nos interesen los sustantivos y verbos, por lo que podríamos programar la eliminación sistemática de determinantes, pronombres o conjunciones que hayan sobrevivido a los filtros anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1258e86f",
   "metadata": {},
   "source": [
    "### **Utilizando el corpus de tweets del MundoToday, realizar un ejemplo práctico de preprocesamiento de texto utilizando spaCy**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f0a76a",
   "metadata": {},
   "source": [
    "NOTA: Realmente el ejercicio exige el uso de Google Collab, pero al tratarse del procesamiento de un CSV pequeño me voy a permitir el lujo de hacerlo en un jupyter notebook en local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8093f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas spacy\n",
    "!{sys.executable} -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc84380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos los imports necesarios antes de nada\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed881271",
   "metadata": {},
   "source": [
    "-> _Para facilitar todo vamos a colocar el archivo ``corpus_mundo_today.csv`` en la misma ruta del jupyter notebook._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69bbab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A continuación cargamos los datos\n",
    "try:\n",
    "    df = pd.read_csv('corpus_mundo_today.csv', delimiter='|')\n",
    "except FileNotFoundError:\n",
    "    print(\"¡Error! Ruta incorrecta. Asegúrate de que el archivo 'corpus_mundo_today.csv' está en el directorio correcto.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d2c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo siguiente será cargar el modelo de spaCy\n",
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "except OSError:\n",
    "    print(\"El modelo no está instalado. Ejecuta en tu terminal: python -m spacy download es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d865c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la funcion de procesamiento de texto, con los requisitos indicados\n",
    "def preprocesar_texto(texto):\n",
    "    doc = nlp(texto)\n",
    "    tokens_limpios = []\n",
    "    \n",
    "    categorias_prohibidas = {'DET', 'PRON', 'CCONJ', 'SCONJ'}\n",
    "    \n",
    "    for token in doc:\n",
    "        if (not token.is_punct and \n",
    "            not token.is_space and \n",
    "            not token.is_stop and \n",
    "            token.pos_ not in categorias_prohibidas):\n",
    "            \n",
    "            tokens_limpios.append(token.lemma_.lower())\n",
    "            \n",
    "    return \" \".join(tokens_limpios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e86300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por último ejecutamos la función\n",
    "if 'df' in locals():\n",
    "    print(\"Procesando...\")\n",
    "    df['texto_procesado'] = df['texto'].apply(preprocesar_texto)\n",
    "    \n",
    "    print(\"\\n--- Resultado ---\")\n",
    "    print(df[['texto', 'texto_procesado']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c854c58",
   "metadata": {},
   "source": [
    "-> _Alternativa: para ver el archivo completo sin depender de la limpieza visual de ``pandas`` ejecutaremos el siguiente bloque de código_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e26819",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('resultado_procesado.csv', index=False, sep='|')\n",
    "print(\"Archivo guardado. Ábrelo con Excel o un editor de texto.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
