{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a6ccc82",
   "metadata": {},
   "source": [
    "# Tarea 3.3 - Modelos de Inteligencia Artificial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab305912",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy spacy scikit-learn gensim\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4d1aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dad010",
   "metadata": {},
   "source": [
    "Cargamos los datos después de las importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb781f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'corpus_mundo_today.csv',\n",
    "    sep=r'\\|\\|',\n",
    "    engine='python',\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "df.drop(columns=[\"tema\"], inplace=True)\n",
    "columnas = df['título'] + \". \" + df['texto']\n",
    "docs_list = columnas.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35913166",
   "metadata": {},
   "source": [
    "Posteriormente normalizamos los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583db8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def Normalization(docs_list):\n",
    "    \"\"\"Normaliza documentos: tokenización, eliminación stopwords, lematización y filtrado POS\"\"\"\n",
    "    corpus_limpio = []\n",
    "    for documento in nlp.pipe(docs_list, disable=[\"ner\", \"parser\"]):\n",
    "        lista_tokens = [\n",
    "            token.lemma_.lower().strip()\n",
    "            for token in documento\n",
    "            if not token.is_punct\n",
    "            and not token.is_stop\n",
    "            and token.pos_ in ['NOUN', 'PROPN', 'VERB', 'ADV', 'ADJ']\n",
    "            and token.text.strip() != \"\"\n",
    "        ]\n",
    "        corpus_limpio.append(\" \".join(lista_tokens))\n",
    "    return corpus_limpio\n",
    "\n",
    "corpus = Normalization(docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618dcbc6",
   "metadata": {},
   "source": [
    "#### 6. **Crear una función que dada una lista de documentos (*corpus*) tokenizados, elimine del corpus aquellos tokens que aparecen menos de 'N' veces (N=10) en el corpus**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "* **input**: 'N' -> Parámetro que nos indica el número mínimo de apariciones de la palabra en el corpus.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4d10a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo documento filtrado (tokens con freq >= 10):\n",
      "gobierno español sumar puigdemont gobierno españa puigdemont semana sumar líder cataluña puigdemont asegurar catalán quedar líder año seguir año puigdemont hacer él semana...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def drop_less_frecuency_words(corpus, n):\n",
    "    \"\"\"Elimina tokens que aparecen menos de n veces en el corpus\"\"\"\n",
    "    # Primero contabilizamos todas las apariciones\n",
    "    all_tokens = []\n",
    "    for doc in corpus:\n",
    "        all_tokens.extend(doc.split())\n",
    "    \n",
    "    frecuencias = Counter(all_tokens)\n",
    "\n",
    "    # Mantenemos solo tokens con frecuencia mayor o igual a n\n",
    "    tokens_validos = {token for token, freq in frecuencias.items() if freq >= n}\n",
    "    \n",
    "    # Por último filtramos corpus\n",
    "    corpus_filtrado = []\n",
    "    for doc in corpus:\n",
    "        tokens_filtrados = [token for token in doc.split() if token in tokens_validos]\n",
    "        corpus_filtrado.append(\" \".join(tokens_filtrados))\n",
    "    \n",
    "    return corpus_filtrado\n",
    "\n",
    "corpus = drop_less_frecuency_words(corpus, 10)\n",
    "print(\"Ejemplo documento filtrado (tokens con freq >= 10):\")\n",
    "print(corpus[0][:200] + \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7c3a86",
   "metadata": {},
   "source": [
    "#### 7. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras en ONE-HOT-ENCODE con Gensim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39467c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de tokens únicos: 134\n",
      "\n",
      "Primeros 10 tokens del diccionario:\n",
      "0: asegurar\n",
      "1: año\n",
      "2: cataluña\n",
      "3: catalán\n",
      "4: españa\n",
      "5: español\n",
      "6: gobierno\n",
      "7: hacer\n",
      "8: líder\n",
      "9: puigdemont\n",
      "\n",
      "BoW del primer documento (One-Hot):\n",
      "[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2), (7, 1), (8, 2), (9, 4)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tokenizado = [doc.split() for doc in corpus]\n",
    "\n",
    "# Creamos el diccionario de términos\n",
    "diccionario = corpora.Dictionary(corpus_tokenizado)\n",
    "\n",
    "print(f\"Número de tokens únicos: {len(diccionario)}\")\n",
    "print(\"\\nPrimeros 10 tokens del diccionario:\")\n",
    "for i, token in list(diccionario.items())[:10]:\n",
    "    print(f\"{i}: {token}\")\n",
    "\n",
    "# Convertimos a Bag of Words\n",
    "corpus_bow = [diccionario.doc2bow(doc) for doc in corpus_tokenizado]\n",
    "\n",
    "print(\"\\nBoW del primer documento (One-Hot):\")\n",
    "print(corpus_bow[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e9fa7",
   "metadata": {},
   "source": [
    "#### 8. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras aplicando el TF-IDF con Scikit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17e99f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TF-IDF Matrix:\n",
      "Shape: (52, 134)\n",
      "Vocabulario: 134 términos\n",
      "\n",
      "Primeros 10 términos del vocabulario:\n",
      "gobierno: índice 54\n",
      "español: índice 44\n",
      "sumar: índice 121\n",
      "puigdemont: índice 106\n",
      "españa: índice 43\n",
      "semana: índice 118\n",
      "líder: índice 75\n",
      "cataluña: índice 17\n",
      "asegurar: índice 4\n",
      "catalán: índice 18\n",
      "\n",
      "TF-IDF del primer documento (primeros 10 valores):\n",
      "puigdemont: 0.7460\n",
      "sumar: 0.3078\n",
      "líder: 0.2758\n",
      "semana: 0.2587\n",
      "gobierno: 0.2137\n",
      "año: 0.2036\n",
      "cataluña: 0.1539\n",
      "hacer: 0.1335\n",
      "quedar: 0.1255\n",
      "españa: 0.1124\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"\\n\\nTF-IDF Matrix:\")\n",
    "print(f\"Shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Vocabulario: {len(tfidf_vectorizer.vocabulary_)} términos\")\n",
    "\n",
    "print(\"\\nPrimeros 10 términos del vocabulario:\")\n",
    "vocab = list(tfidf_vectorizer.vocabulary_.items())[:10]\n",
    "for term, idx in vocab:\n",
    "    print(f\"{term}: índice {idx}\")\n",
    "\n",
    "print(\"\\nTF-IDF del primer documento (primeros 10 valores):\")\n",
    "doc_tfidf = tfidf_matrix[0].toarray()[0]\n",
    "indices_ordenados = np.argsort(doc_tfidf)[::-1][:10]\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "for idx in indices_ordenados:\n",
    "    if doc_tfidf[idx] > 0:\n",
    "        print(f\"{feature_names[idx]}: {doc_tfidf[idx]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
